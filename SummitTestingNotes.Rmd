---
title: "Summit Computing Testing Notes"
author: "Chris Mellinger"
date: "6/23/2017"
output: pdf_document
---

# Testing on My Machine.

On my computer, I was able to run 2 iterations at about 210 seconds. Parallelized, this reduced to about 105 seconds.

With 3 iterations (and after clearing all variables and restarting R), time was 122.287 using 3 cores. With 6 iterations, time was 212.75 seconds with 6 cores.

# RC Testing

On the first successful run, 6 iterations took 567.459 seconds, or 9.45 minutes.

On run 2, I specified the correct number of tasks per node to avoid memory problems (24), and then I tried for 50 iterations. It took about 9.5 minutes again, which tells me I was probably doing something wrong before. But that is pretty cool. I am seeing about 4.1 minute averages, so I should maybe figure 5 minutes per iteration.

For the next run, I want to try something bigger, like 500. I need $500 corejobs * 5 minutes/job / 24 cores = 104.2 minutes$. So I need an hour and 45 total. **After run notes.** For some reason, the iterations started slowing down somewhat quickly. Looks like I will need to count on about 350 - 400 seconds per iteration to be safe. Durn.



# Notes on Getting Things to Work

I installed packages from an scompile node:

```
ssh scompile
ml R
export R_LIBS = '/projects/chme2908/R_libs' # set environment variable for R libs
R
```

Then in R:

```{r, eval=F}
inst <- function(pkg) {install.packages(pkg, repos='http://cran.r-project.org')}
inst(pkg1)
# let it run
inst(pkg2)
# let it run
```

Then, there should be packages in that R-libs directory.

To actually make scripts find those packages, add this line:
```{r}
.libPaths(c(.libPaths(), '/projects/chme2908/R_libs'))
```

It has to go anywhere packages need to be loaded: the beginning of the script, and on any clusters intiated as well.

I never got devtools to work in a script, but I could use it on the scompile node.

# Timing Info

On August 8, 2017 I ran 500 iterations with 32 participants and 2 replications. Logged in `big-job_219448.out`, the modeling part took `r 6202.6 / (60^2)` hours.

A test job with 6 iterations, running on 6 cores, 32 ps and 2 reps took 122 seconds.

Generating data for 500 iterations of a larger simulation (64 participants, 8 primes, 8 targets, and 2 of each category) too 9.5593 minutes (573.563s) to accomplish. Extrapolating, 10000 data sets will take `r 573.563 / 500 * 10000 / 60` minutes. This might be infeasible. I think I will pry have to implement the randomness within the parallelized workers.

Trying estimate the task is also proving difficult. For the record, 32 ps with 4 primes and targets is worth 95.72 seconds. It took 290 seconds to estimate 1 iteration of 32 participants, 8 targets, and 4 primes. Estimating the model with 32 ps, 8 and 8, 6963.1 seconds, which is almost 2 hours. Estimating 64 ps with 4 primes and targets took 734 seconds, ~ 12 minutes.

# Timing Info on Summit

Running 12 jobs on 12 cores (1 node), I am averaging about 175 seconds per iteration. Turned out to be 674 seconds for the low variance and 838 seconds for the high variance. This is with 32 ps, 4 primes and 4 targets. Total runtime: 00:06:02.

Running 2 jobs on 2 cores (1 node), with 32 ps and 8 primes and targets, each model seems to take around 5000 seconds. That is ~83 minutes. To piggy-back on this, it took 2:45:26 to run both versions of 2 iterations.

## 8 primes, 8 targets, 20 subjects, fully paralellized, estimating the number of iterations for both high and low variance:

6 iterations (12 cores): 4125s, 69min, 1hr
12 iterations (24 cores): 3760s, 63min, 1hr
12 iterations (24 cores): 3826s, 64min, 1hr
18 iterations (36 cores): 6039s, 100.65min, 1.67hr
24 iterations (48 cores): 8480s, 141min, 2.4hrs
36 iterations (72 cores): 13699s, 228min 3.8hrs

## With MPI

After getting an MPI script operating, I was able to run a 24-iteration (48 total estimations) script with 4 primes and targets, and 20 subjects in less than 25 seconds.

It takes about 65 minutes of core time to run the bigger (8 primes and targets) estimations. To run 10000 iterations of each (20000 estimations), that is $20000 \times 65 = 1300000$ minutes. There are 1440 minutes in a day. Thus, I would need $ 1300000 / 1440 \approx 903$ cores, which is 38 nodes to achieve this in 24 hours. For safety, I will use 40 nodes, which provides 960 cores.

For one of the runs at 8 primes & targets 20 participants, with 960 cores it took 10.3 hours. For the other, it took 7.9 hours.

Okay, as of now, I am getting 30 participants, 8 primes/targets done in 25 minutes. WTH? On a second try, it took 20 minutes. So, we have $10000 * 5 = 50000$ iterations to run in total. Thus, we need $50000 * 25 / 60 = 20833.33$ core hours. With 1000 cores, that implies about 20 hours of run time.

On a run with 84 nodes, I ran over a 12 hour time limit.

A run finished on 2-9-18 in 12.72 hours. This utilized 90 nodes and the "saving data and results" style of aggregation. Next time try putting a time limit of 15 hours on it.